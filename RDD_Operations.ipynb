{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'a', 'c']\n",
      "[('b', 1), ('a', 1), ('c', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Spark Transformation  - map\n",
    "\n",
    "x= sc.parallelize([\"b\", \"a\", \"c\"])\n",
    "y= x.map(lambda z: (z, 1))\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b, a, c\n",
      "(b,1), (a,1), (c,1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "x = ParallelCollectionRDD[0] at parallelize at <console>:28\n",
       "y = MapPartitionsRDD[1] at map at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[1] at map at <console>:29"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// %scala\n",
    "val x= sc.parallelize(Array(\"b\", \"a\", \"c\"))\n",
    "val y= x.map(z => (z,1))\n",
    "println(x.collect().mkString(\", \"))\n",
    "println(y.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 100, 42, 2, 200, 42, 3, 300, 42]\n"
     ]
    }
   ],
   "source": [
    "# Spark Transformation  - flatMap\n",
    "x= sc.parallelize([1,2,3])\n",
    "y= x.flatMap(lambda x: (x, x*100, 42))\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3\n",
      "1, 100, 42, 2, 200, 42, 3, 300, 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "x = ParallelCollectionRDD[0] at parallelize at <console>:28\n",
       "y = MapPartitionsRDD[1] at flatMap at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[1] at flatMap at <console>:29"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// %scala\n",
    "val x= sc.parallelize(Array(1,2,3))\n",
    "val y= x.flatMap(n => Array(n, n*100, 42))\n",
    "println(x.collect().mkString(\", \"))\n",
    "println(y.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 3]\n"
     ]
    }
   ],
   "source": [
    "# Spark Transformation  - filter\n",
    "\n",
    "x= sc.parallelize([1,2,3])\n",
    "y= x.filter(lambda x: x%2 == 1) #keep odd values\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3\n",
      "1, 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "x = ParallelCollectionRDD[2] at parallelize at <console>:31\n",
       "y = MapPartitionsRDD[3] at filter at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[3] at filter at <console>:32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// %scala\n",
    "val x= sc.parallelize(Array(1,2,3))\n",
    "val y= x.filter(n => n%2 == 1)\n",
    "println(x.collect().mkString(\", \"))\n",
    "println(y.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('F', ['Fred']), ('J', ['John', 'James']), ('A', ['Anna'])]\n"
     ]
    }
   ],
   "source": [
    "# Spark Transformation  - groupBy\n",
    "\n",
    "x= sc.parallelize(['John', 'Fred', 'Anna', 'James'])\n",
    "y= x.groupBy(lambda w: w[0])\n",
    "print ([(k, list(v)) for (k, v) in y.collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%scala\n",
    "val x= sc.parallelize(\n",
    "Array(\"John\", \"Fred\", \"Anna\", \"James\"))\n",
    "val y= x.groupBy(w => w.charAt(0))\n",
    "println(y.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 5), ('B', 4), ('A', 3), ('A', 2), ('A', 1)]\n",
      "[('B', [5, 4]), ('A', [3, 2, 1])]\n"
     ]
    }
   ],
   "source": [
    "# Spark Transformation  - groupByKey\n",
    "\n",
    "\n",
    "x= sc.parallelize([('B',5),('B',4),('A',3),('A',2),('A',1)])\n",
    "y= x.groupByKey()\n",
    "print(x.collect())\n",
    "print(list((j[0], list(j[1])) for j in y.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// %scala\n",
    "val x= sc.parallelize(Array(('B',5),('B',4),('A',3),('A',2),('A',1)))\n",
    "val y= x.groupByKey()\n",
    "println(x.collect().mkString(\", \"))\n",
    "println(y.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReduceByKey VS groupByKey..\n",
    "\n",
    "// %scala\n",
    "val words = Array(\"one\", \"two\", \"two\", \"three\", \"three\", \"three\")\n",
    "val wordPairsRDD= sc.parallelize(words).map(word => (word, 1))\n",
    "val wordCountsWithReduce= wordPairsRDD\n",
    ".reduceByKey(_ + _)\n",
    ".collect()\n",
    "\n",
    "val wordCountsWithGroup= wordPairsRDD\n",
    ".groupByKey()\n",
    ".map(t => (t._1, t._2.sum))\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3]]\n",
      "[[1, 42], [5, 42]]\n"
     ]
    }
   ],
   "source": [
    "# Spark Transformation  -  mapPartitions()\n",
    "\n",
    "x= sc.parallelize([1,2,3], 2)\n",
    "def f(iterator):yield sum(iterator); yield 42\n",
    "y= x.mapPartitions(f)\n",
    "# glom() flattens elements on the same partition\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// %scala\n",
    "val x= sc.parallelize(Array(1,2,3), 2)\n",
    "def f(i:Iterator[Int])={ (i.sum,42).productIterator}\n",
    "val y= x.mapPartitions(f)\n",
    "// glom() flattens elements on the same partition\n",
    "val xOut= x.glom().collect()\n",
    "val yOut= y.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3]]\n",
      "[[(0, 1)], [(1, 5)]]\n"
     ]
    }
   ],
   "source": [
    "# Spark Transformation  - mapPartitionsWithIndex\n",
    "\n",
    "x= sc.parallelize([1,2,3], 2)\n",
    "def f(partitionIndex, iterator):yield (partitionIndex, sum(iterator))\n",
    "y= x.mapPartitionsWithIndex(f)\n",
    "# glom() flattens elements on the same partition\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// %scala\n",
    "val x= sc.parallelize(Array(1,2,3), 2)\n",
    "def f(partitionIndex:Int, i:Iterator[Int]) = {\n",
    "  (partitionIndex, i.sum).productIterator\n",
    "  }\n",
    "val y= x.mapPartitionsWithIndex(f)\n",
    "// glom() flattens elements on the same partition\n",
    "val xOut= x.glom().collect()\n",
    "val yOut= y.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "[2, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# Spark Transformation  - sample\n",
    "\n",
    "x= sc.parallelize([1, 2, 3, 4, 5])\n",
    "y= x.sample(False, 0.4, 42)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// %scala\n",
    "val x= sc.parallelize(Array(1, 2, 3, 4, 5))\n",
    "val y= x.sample(false, 0.4)\n",
    "// omitting seed will yield different output\n",
    "println(y.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3], [3, 4]]\n"
     ]
    }
   ],
   "source": [
    "# Spark Transformation  - union\n",
    "\n",
    "\n",
    "x= sc.parallelize([1,2,3], 2)\n",
    "y= sc.parallelize([3,4], 1)\n",
    "z= x.union(y)\n",
    "print(z.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Error parsing magics!\n",
       "Message: Magics [scala] do not exist!\n",
       "StackTrace: "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// %scala\n",
    "val x= sc.parallelize(Array(1,2,3), 2)\n",
    "val y= sc.parallelize(Array(3,4), 1)\n",
    "val z= x.union(y)\n",
    "val zOut= z.glom().collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
